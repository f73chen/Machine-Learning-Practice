{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import to_categorical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    df = pd.read_csv(filepath, header = None, delim_whitespace = True)\n",
    "    return df.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Load a list of files into a 3D array of [samples, timesteps, features]\n",
    "# One sample is one window of the time series data\n",
    "# Each window has 128 time steps\n",
    "# Each time step has 9 features\n",
    "\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    \n",
    "    # Stack group so features are in the 3rd dimension\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load a dataset group (train or test)\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    filenames = list()\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\n",
    "    x = load_group(filenames, filepath)\n",
    "    y = load_file(prefix + group + f'/y_{group}.txt')\n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Split dataset into train and test x and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    train_x, train_y = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "    print(train_x.shape, train_y.shape)\n",
    "    test_x, test_y = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "    print(test_x.shape, test_y.shape)\n",
    "\n",
    "    # Zero-offset class values\n",
    "    train_y = train_y - 1\n",
    "    test_y = test_y - 1\n",
    "\n",
    "    # One-hot encode y\n",
    "    train_y = to_categorical(train_y)\n",
    "    test_y = to_categorical(test_y)\n",
    "    print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_x, train_y, test_x, test_y = load_dataset()\n",
    "n_timesteps, n_features, n_outputs =train_x.shape[1], train_x.shape[2], train_y.shape[1]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train and evaluate a model\n",
    "def evaluate_model(train_x, train_y, test_x, test_y):\n",
    "    verbose = 0\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorial_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('old': conda)"
  },
  "interpreter": {
   "hash": "474e7d79e085ddcf2ecdd2326435fa1e407305f167ada9ec6168e0702bb8ca10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}